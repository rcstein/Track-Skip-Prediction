---
title: "An In-Depth Guide to catgeorization problems"
output:
  html_document:
    df_print: paged
---
Machine Learning can be a useful tool for many sorts of problems. One of the most common type of problem is categorization. This tutorial will give an in-depth approach to solving these types of problems. For the purpose of this tutorial we will be using the spotify sequential skip prediction data set which can be found here {https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge}

##Data Loading and Cleaning pt. 1
The first thing to do is to load the data which in this case can be done using the read.csv() function. You can then view the loaded data using View() and you can see the structure of the data using str().
```{r}
#assuming the dcsv files are in your rstudio project folder
data <- read.csv("log_mini.csv")
View(data)
str(data)
```
To keep this simple our goal will be to predict whether or not a song will be skipped. This information can be seen in the 'not_skipped' column. Now you might notice that this column contains the test words 'true' and 'false'. Unfortunately most if not all Machine learning tools cannot work with test inputs. Thus we need to convert this to a binary representation. This can be done pretty easily in R. (note:specific columns in a dataframe can be accessed with the $ operator followed by the name of the column)
```{r}
data$not_skipped = as.logical(data$not_skipped)
```
If you go back to the view of data you can now see that the 'not_skipped' column contains 'TRUE' and 'FALSE'. This may seem like text but under the hood R represents these as 0s and 1s. You'll probably also notice that there are a number of other columns with text 'true' and 'false values. You can go ahead and convert these as well.
```{r}
data$skip_1 = as.logical(data$skip_1)
data$skip_2 = as.logical(data$skip_2)
data$skip_3 = as.logical(data$skip_3)
data$hist_user_behavior_is_shuffle = as.logical(data$hist_user_behavior_is_shuffle)
data$premium = as.logical(data$premium)
str(data)
```

The as.logical function can also handle binary representations and while not always necessary, for the sake of consistency you can convert the binary columns to logical as well.
```{r}
data$context_switch = as.logical(data$context_switch)
data$no_pause_before_play <- as.logical(data$no_pause_before_play)
data$short_pause_before_play <- as.logical(data$short_pause_before_play)
data$long_pause_before_play <- as.logical(data$long_pause_before_play)
data$hist_user_behavior_n_seekfwd <- as.logical(data$hist_user_behavior_n_seekfwd)
data$hist_user_behavior_n_seekback <- as.logical(data$hist_user_behavior_n_seekback)
str(data)
```
## Logistic Regression

Now that we have a bunch of numeric data we can start modeling! One of the most basic tools for this is logistic regression. 
{https://en.wikipedia.org/wiki/Logistic_regression}
`


```{r}
play_data <- read.csv("log_mini.csv")
track_data <- read.csv("tf_mini.csv")
track_data$track_id = as.character(track_data$track_id)
```
```{r}
library(tidyr)
library(dplyr)
library(e1071)
library(lubridate)
library(glmnet)
library(ggplot2)

```
```{r}
combined_data <- full_join(play_data,track_data,by = c("track_id_clean" = "track_id"))


#convert categorical integers to factors for automated dummy-coding
combined_data$context_type <- as.factor(combined_data$context_type)
combined_data$hist_user_behavior_reason_start <- as.factor(combined_data$hist_user_behavior_reason_start)
combined_data$hist_user_behavior_reason_end <- as.factor(combined_data$hist_user_behavior_reason_end)
combined_data$key <- as.factor(combined_data$key)
combined_data$time_signature <- factor(combined_data$time_signature)
combined_data$mode <- factor(combined_data$mode)

#convert T/F values 
combined_data$context_switch = as.logical(combined_data$context_switch)
combined_data$skip_1 <- as.logical(combined_data$skip_1)
combined_data$skip_2 <- as.logical(combined_data$skip_2)
combined_data$skip_3 <- as.logical(combined_data$skip_3)
combined_data$not_skipped <- as.logical(combined_data$not_skipped)
combined_data$no_pause_before_play <- as.logical(combined_data$no_pause_before_play)
combined_data$short_pause_before_play <- as.logical(combined_data$short_pause_before_play)
combined_data$long_pause_before_play <- as.logical(combined_data$long_pause_before_play)
combined_data$hist_user_behavior_n_seekfwd <- as.logical(combined_data$hist_user_behavior_n_seekfwd)
combined_data$hist_user_behavior_n_seekback <- as.logical(combined_data$hist_user_behavior_n_seekback)
combined_data$hist_user_behavior_is_shuffle <- as.logical(combined_data$hist_user_behavior_is_shuffle)
combined_data$premium <- as.logical(combined_data$premium)

#standardize continuous numerical variables
combined_data$place_in_session = combined_data$session_position / combined_data$session_length
combined_data$place_in_day = combined_data$hour_of_day/24

d = lubridate::as_date(combined_data$date)
combined_data$month = month(d)
combined_data$year = year(d)
combined_data$day = day(d)
combined_data$day_of_week = weekdays(d)

dom = days_in_month(combined_data$month)

weekday_to_int<- function(x){
  w = c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')
  i = match(x,w) -1
  return(i)
}

weekday_to_int(combined_data$day_of_week[[1]])
p_o_w1 = sapply(combined_data$day_of_week, weekday_to_int)
combined_data$place_in_week = p_o_w1/7 + combined_data$hour_of_day / (7*24)
combined_data$place_in_month = (combined_data$day-1) / dom + (combined_data$hour_of_day /(dom*24))
combined_data$day_of_week = as.factor(combined_data$day_of_week)


combined_data$since_release = combined_data$year -combined_data$release_year
# separate out normalized play and track data; separate out response variable (skipped T/F)
combined_data$month = as.factor(combined_data$month)
combined_data$day = as.integer(combined_data$day)
combined_data$year = as.factor(combined_data$year)



# convert strings to booleans
combined_data <- rapply(combined_data,scale,c("numeric"),how="replace")
str(combined_data)
```
Question: how do we handle the session position / session lengths? Could represent with fraction, or track time elapsed in session vs. duration of song?

```{r}
X=subset(combined_data,select=-c(session_id,track_id_clean,date,year,skip_1,skip_2,skip_3,not_skipped))

cl = sapply(X, class)
ps = sapply(cl, function(x){
  return(ifelse(x=='factor'||x=='logical','-1',''))
})

f =  paste(colnames(X),ps,collapse = " + ")
form = as.formula(paste(c('~ ',f),collapse = ''))

X_dum = model.matrix(form,data=X)
Y <- combined_data$not_skipped

set.seed(10201998)
test = sample(length(Y),(length(Y)%/%10))

Y_test = Y[test]
Y_train = Y[-test]
X_test = X_dum[test,]
X_train = X_dum[-test,]
```

```{r}
glm_lasso = cv.glmnet(X_train,Y_train,nfolds=10,alpha=1,family='binomial')
preds_lasso = predict(glm_lasso,newx=X_test,s=c(glm_lasso$lambda.min),type='response')
pred_tf_lasso = preds_lasso>.5
acc_frame_lasso = tibble(pred=pred_tf_lasso,true=Y_test)
conf_mat_lasso = (acc_frame_lasso %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_lasso= c(conf_mat_lasso[1,1]/sum(conf_mat_lasso[,1]),conf_mat_lasso[2,2]/sum(conf_mat_lasso[,2]))
recall_lasso = c(conf_mat_lasso[1,1]/sum(conf_mat_lasso[1,]),conf_mat_lasso[2,2]/sum(conf_mat_lasso[2,]))
acc_mat_lasso = pred_tf_lasso == Y_test
acc_lasso = sum(acc_mat_lasso)/length(acc_mat_lasso) * 100
print(list(conf_mat=conf_mat_lasso,precision=precision_lasso,recall=recall_lasso,accuracy=acc_lasso,lambda=glm_lasso$lambda.min))


lasso_roc = glmnet::roc.glmnet(glm_lasso,newy=Y_test,newx=X_test)
ggplot(data = lasso_roc,aes(x=FPR,y=TPR)) + geom_point()
asses_lasso = glmnet::assess.glmnet(glm_lasso,newy=Y_test,newx=X_test)
auc_lasso = asses_lasso$auc
```

```{r}
glm_ridge = cv.glmnet(X_train,Y_train,nfolds=10,alpha=0,family='binomial')
preds_ridge = predict(glm_ridge,newx=X_test,s=c(glm_ridge$lambda.min),type='response')
pred_tf_ridge = preds_ridge>.5
acc_frame_ridge = tibble(pred=pred_tf_ridge,true=Y_test)
conf_mat_ridge = (acc_frame_ridge %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_ridge= c(conf_mat_ridge[1,1]/sum(conf_mat_ridge[,1]),conf_mat_ridge[2,2]/sum(conf_mat_ridge[,2]))
recall_ridge = c(conf_mat_ridge[1,1]/sum(conf_mat_ridge[1,]),conf_mat_ridge[2,2]/sum(conf_mat_ridge[2,]))
acc_mat_ridge = pred_tf_ridge == Y_test
acc_ridge = sum(acc_mat_ridge)/length(acc_mat_ridge) * 100
print(list(conf_mat=conf_mat_ridge,precision=precision_ridge,recall=recall_ridge,accuracy=acc_ridge,lambda=glm_ridge$lambda.min))

ridge_roc = glmnet::roc.glmnet(glm_ridge,newy=Y_test,newx=X_test)
ggplot(data = ridge_roc,aes(x=FPR,y=TPR)) + geom_point()
asses_ridge = glmnet::assess.glmnet(glm_ridge,newy=Y_test,newx=X_test)
asses_ridge$auc
auc_ridge = asses_ridge$auc
```


```{r}
naive_bayes <- naiveBayes(x=X_train,y=Y_train)
preds_nb = predict(naive_bayes,newdata=as.data.frame(X_test))
acc_frame_nb = tibble(pred=preds_nb,true=Y_test)
conf_mat_nb = (acc_frame_nb %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_nb= c(conf_mat_nb[1,1]/sum(conf_mat_nb[,1]),conf_mat_nb[2,2]/sum(conf_mat_nb[,2]))
recall_nb = c(conf_mat_nb[1,1]/sum(conf_mat_nb[1,]),conf_mat_nb[2,2]/sum(conf_mat_nb[2,]))
acc_mat_nb = preds_nb == Y_test
acc_nb = sum(acc_mat_nb)/length(acc_mat_nb) * 100
print(list(conf_mat=conf_mat_nb,precision=precision_nb,recall=recall_nb,accuracy=acc_nb))
```
```{r}
library(MASS)
library(ROCR)
lda = lda(x=X_train,grouping=Y_train)
preds_lda = predict(lda,newdata=as.data.frame(X_test))
pred_tf_lda = preds_lda$class
acc_frame_lda = tibble(pred=pred_tf_lda,true=Y_test)
conf_mat_lda = (acc_frame_lda %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_lda= c(conf_mat_lda[1,1]/sum(conf_mat_lda[,1]),conf_mat_lda[2,2]/sum(conf_mat_lda[,2]))
recall_lda = c(conf_mat_lda[1,1]/sum(conf_mat_lda[1,]),conf_mat_lda[2,2]/sum(conf_mat_lda[2,]))
acc_mat_lda = pred_tf_lda == Y_test
acc_lda = sum(acc_mat_lda)/length(acc_mat_lda) * 100
print(list(conf_mat=conf_mat_lda,precision=precision_lda,recall=recall_lda,accuracy=acc_lda))

preds_lda$posterior[,2]

rocr_lda = prediction(preds_lda$posterior[,2],Y_test)
assess_lda = performance(rocr_lda,measure = 'auc')
auc_lda = as.numeric(assess_lda@y.values[[1]])
```
```{r}
qda = qda(x=X_train,grouping=Y_train)
preds_qda = predict(qda,newdata=as.data.frame(X_test))
pred_tf_qda = preds_qda$class
acc_frame_qda = tibble(pred=pred_tf_qda,true=Y_test)
conf_mat_qda = (acc_frame_qda %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_qda= c(conf_mat_qda[1,1]/sum(conf_mat_qda[,1]),conf_mat_qda[2,2]/sum(conf_mat_qda[,2]))
recall_qda = c(conf_mat_qda[1,1]/sum(conf_mat_qda[1,]),conf_mat_qda[2,2]/sum(conf_mat_qda[2,]))
acc_mat_qda = pred_tf_qda == Y_test
acc_qda = sum(acc_mat_qda)/length(acc_mat_qda) * 100
print(list(conf_mat=conf_mat_qda,precision=precision_qda,recall=recall_qda,accuracy=acc_qda))
```

```{r}
km2 = kmeans(X_dum,centers = 2,iter.max = 30)
preds_km = km2$cluster
pred_tf_km = as.logical(preds_km-1)
acc_frame_km = tibble(pred=pred_tf_km,true=Y)
conf_mat_km = (acc_frame_km %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_km= c(conf_mat_km[1,1]/sum(conf_mat_km[,1]),conf_mat_km[2,2]/sum(conf_mat_km[,2]))
recall_km = c(conf_mat_km[1,1]/sum(conf_mat_km[1,]),conf_mat_km[2,2]/sum(conf_mat_km[2,]))
acc_mat_km = pred_tf_km == Y
acc_km = sum(acc_mat_km)/length(acc_mat_km) * 100
print(list(conf_mat=conf_mat_km,precision=precision_km,recall=recall_km,accuracy=acc_km))

```


```{r}
aucs = c(auc_lasso,auc_lda)
aucs
preds_avg = (preds_lasso*auc_lasso + preds_lda$posterior[,2]*auc_lda)/ sum(aucs)

rocr_avg = prediction(preds_avg,Y_test)
assess_avg = performance(rocr_avg,measure = 'auc')
auc_avg = as.numeric(assess_avg@y.values[[1]])
auc_avg


pred_tf_avg = preds_avg>.5
acc_frame_avg = tibble(pred=pred_tf_avg,true=Y_test)
conf_mat_avg = (acc_frame_avg %>% group_by(true,pred) %>% count())$n %>% matrix(ncol = 2,nrow=2)
precision_avg= c(conf_mat_avg[1,1]/sum(conf_mat_avg[,1]),conf_mat_avg[2,2]/sum(conf_mat_avg[,2]))
recall_avg = c(conf_mat_avg[1,1]/sum(conf_mat_avg[1,]),conf_mat_avg[2,2]/sum(conf_mat_avg[2,]))
acc_mat_avg = pred_tf_avg == Y_test
acc_avg = sum(acc_mat_avg)/length(acc_mat_avg) * 100
print(list(conf_mat=conf_mat_avg,precision=precision_avg,recall=recall_avg,accuracy=acc_avg))
```

